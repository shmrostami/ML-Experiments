{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ec58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting (type 'exit' to quit):\n",
      "English\n",
      "\n",
      "The answer is 2.\n",
      "English\n",
      "\n",
      "The result of adding 3 to 2 is 5.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Chat with in memory history\n",
    "import os\n",
    "from typing import Dict\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Global configuration\n",
    "# -------------------------------\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://127.0.0.1:11434\")\n",
    "\n",
    "\n",
    "def setup_environment() -> None:\n",
    "    \"\"\"Ensure local Ollama traffic bypasses proxies.\"\"\"\n",
    "    os.environ.setdefault(\"no_proxy\", \"127.0.0.1,localhost\")\n",
    "    os.environ.setdefault(\"HTTPX_NO_PROXY\", \"127.0.0.1,localhost\")\n",
    "\n",
    "\n",
    "def create_chat_chain(model_name: str):\n",
    "    \"\"\"\n",
    "    Create a conversational chain with in-memory history.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the Ollama model to use.\n",
    "\n",
    "    Returns:\n",
    "        chain_with_history: RunnableWithMessageHistory instance\n",
    "        history_store: Dict holding chat histories by session_id\n",
    "    \"\"\"\n",
    "    # Initialize Ollama LLM\n",
    "    llm = OllamaLLM(model=model_name, base_url=OLLAMA_URL, timeout=30)\n",
    "\n",
    "    # Define prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"You are a helpful AI assistant. \"\n",
    "         \"Respond in Persian if the user speaks Persian, otherwise in English.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"{content}\")\n",
    "    ])\n",
    "\n",
    "    # Base chain (prompt ‚Üí model)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # Session-based history storage\n",
    "    history_store: Dict[str, InMemoryChatMessageHistory] = {}\n",
    "\n",
    "    def get_session_history(session_id: str):\n",
    "        \"\"\"Retrieve or create chat history for the session.\"\"\"\n",
    "        if session_id not in history_store:\n",
    "            history_store[session_id] = InMemoryChatMessageHistory()\n",
    "        return history_store[session_id]\n",
    "\n",
    "    # Add history management\n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"content\",\n",
    "        history_messages_key=\"messages\"\n",
    "    )\n",
    "\n",
    "    return chain_with_history, history_store\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run an interactive chat with persistent in-memory history.\"\"\"\n",
    "    setup_environment()\n",
    "\n",
    "    model_name = \"llama3.2:3b\"\n",
    "    chain_with_history, history_store = create_chat_chain(model_name)\n",
    "\n",
    "    print(\"Start chatting (type 'exit' to quit):\")\n",
    "    session_id = \"default_session\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\">> \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Run the chain\n",
    "            response = chain_with_history.invoke(\n",
    "                {\"content\": user_input, \"messages\": []},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "\n",
    "            print(response.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "\n",
    "        # Debug: show current history (optional, can be removed)\n",
    "        # print(\"History:\", history_store[session_id].messages)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e79836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:04:46,256 - INFO - Created or verified history directory: chat_history\n",
      "2025-09-04 17:04:46,257 - INFO - üöÄ Initializing model: llama3.2:3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:04:46,808 - INFO - Initialized LLM: llama3.2:3b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:04:54,026 - INFO - Initialized history file for session default_session: chat_history\\chat_history_default_session.json\n",
      "2025-09-04 17:04:58,598 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-09-04 17:04:59,596 - INFO - ‚úÖ Generated response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ÿæ€åÿ¥ ÿ®€åŸÜ€å ŸÖ€å ÿ¥ŸàÿØ ⁄©Ÿá 1+1 = 2 ÿßÿ≥ÿ™.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:05:30,937 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-09-04 17:05:32,246 - INFO - ‚úÖ Generated response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿßÿ≤ ÿ∑ÿ±€åŸÇ ÿß€åŸÜ to√°nÿå ÿ®Ÿá ÿß€åŸÜ ŸÜÿ™€åÿ¨Ÿá ŸÖ€å ÿ±ÿ≥€åŸÖ ⁄©Ÿá 2 + 3 = 5 ÿßÿ≥ÿ™.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:05:49,195 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-09-04 17:05:49,680 - INFO - ‚úÖ Generated response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿ¢ÿ¥ŸÜÿß ŸÖ€åÿ¥ŸÖ. goodbye.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Chat with saving history in Json\n",
    "import os\n",
    "from typing import Dict\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import FileChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Global Ollama configuration\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://127.0.0.1:11434\")\n",
    "HISTORY_DIR = \"chat_history\"\n",
    "\n",
    "def setup_environment() -> None:\n",
    "    \"\"\"Ensure local Ollama traffic bypasses proxies and create history directory.\"\"\"\n",
    "    os.environ.setdefault(\"no_proxy\", \"127.0.0.1,localhost\")\n",
    "    os.environ.setdefault(\"HTTPX_NO_PROXY\", \"127.0.0.1,localhost\")\n",
    "    os.makedirs(HISTORY_DIR, exist_ok=True)  # Create history directory if it doesn't exist\n",
    "\n",
    "def create_chat_chain(model_name: str) -> tuple[RunnableWithMessageHistory, Dict]:\n",
    "    \"\"\"\n",
    "    Create a conversational chain with persistent file-based history.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the Ollama model to use.\n",
    "\n",
    "    Returns:\n",
    "        chain_with_history: RunnableWithMessageHistory instance\n",
    "        history_store: Dict holding chat histories by session_id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Ollama LLM\n",
    "        llm = OllamaLLM(model=model_name, base_url=OLLAMA_URL, timeout=30)\n",
    "\n",
    "        # Define prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a helpful AI assistant. \"\n",
    "             \"Respond in Persian if the user speaks Persian, otherwise in English.\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\"human\", \"{content}\")\n",
    "        ])\n",
    "\n",
    "        # Base chain (prompt ‚Üí model)\n",
    "        chain = prompt | llm\n",
    "\n",
    "        # Session-based history storage\n",
    "        history_store: Dict[str, FileChatMessageHistory] = {}\n",
    "\n",
    "        def get_session_history(session_id: str) -> FileChatMessageHistory:\n",
    "            \"\"\"Retrieve or create chat history for the session.\"\"\"\n",
    "            history_file = os.path.join(HISTORY_DIR, f\"chat_history_{session_id}.json\")\n",
    "            if session_id not in history_store:\n",
    "                history_store[session_id] = FileChatMessageHistory(history_file)\n",
    "            return history_store[session_id]\n",
    "\n",
    "        # Add history management\n",
    "        chain_with_history = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"content\",\n",
    "            history_messages_key=\"messages\"\n",
    "        )\n",
    "\n",
    "        return chain_with_history, history_store\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run an interactive chat with persistent file-based history.\"\"\"\n",
    "    setup_environment()\n",
    "\n",
    "    model_name = \"llama3.2:3b\"\n",
    "\n",
    "    # Create chat chain and history store\n",
    "    try:\n",
    "        chain_with_history, history_store = create_chat_chain(model_name)\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    print(\"Start chatting (type 'exit' to quit):\")\n",
    "    session_id = \"default_session\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\">> \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Run the chain\n",
    "            response = chain_with_history.invoke(\n",
    "                {\"content\": user_input, \"messages\": []},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "\n",
    "            print(response.strip())\n",
    "\n",
    "            # Debug: show current history (optional, can be removed)\n",
    "            # print(\"History:\", history_store[session_id].messages)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e2bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:11:23,867 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English.\n",
      "\n",
      "The answer is 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:11:46,724 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English.\n",
      "\n",
      "The result was 2. Adding 3 gives us: 2 + 3 = 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:12:05,429 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿÆÿØÿßÿ≠ÿßŸÅÿ∏! (Goodbye!)\n",
      "\n",
      "Note: I responded in Persian to acknowledge your initial message, but switched back to English as per our initial agreement since the majority of our conversation was in English. If you'd like to switch back to Persian or have any further questions, feel free to ask!\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Chat with saving history in SQLite\n",
    "import os\n",
    "from typing import Dict\n",
    "from sqlalchemy import create_engine\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Global configuration\n",
    "# -------------------------------\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://127.0.0.1:11434\")\n",
    "DB_PATH = \"chat_history.db\"  # SQLite database file\n",
    "engine = create_engine(f\"sqlite:///{DB_PATH}\")\n",
    "\n",
    "\n",
    "def setup_environment() -> None:\n",
    "    \"\"\"Ensure local Ollama traffic bypasses proxies.\"\"\"\n",
    "    os.environ.setdefault(\"no_proxy\", \"127.0.0.1,localhost\")\n",
    "    os.environ.setdefault(\"HTTPX_NO_PROXY\", \"127.0.0.1,localhost\")\n",
    "\n",
    "\n",
    "def create_chat_chain(model_name: str):\n",
    "    \"\"\"\n",
    "    Create a conversational chain with SQLite-backed history.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the Ollama model to use.\n",
    "\n",
    "    Returns:\n",
    "        chain_with_history: RunnableWithMessageHistory instance\n",
    "        history_store: Dict holding chat histories by session_id\n",
    "    \"\"\"\n",
    "    # Initialize Ollama LLM\n",
    "    llm = OllamaLLM(model=model_name, base_url=OLLAMA_URL, timeout=30)\n",
    "\n",
    "    # Define prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"You are a helpful AI assistant. \"\n",
    "         \"Respond in Persian if the user speaks Persian, otherwise in English.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"{content}\")\n",
    "    ])\n",
    "\n",
    "    # Base chain (prompt ‚Üí model)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # Session-based history storage\n",
    "    history_store: Dict[str, SQLChatMessageHistory] = {}\n",
    "\n",
    "    def get_session_history(session_id: str):\n",
    "        \"\"\"Retrieve or create chat history for the session.\"\"\"\n",
    "        if session_id not in history_store:\n",
    "            history_store[session_id] = SQLChatMessageHistory(\n",
    "                session_id=session_id,\n",
    "                connection=engine\n",
    "            )\n",
    "        return history_store[session_id]\n",
    "\n",
    "    # Add history management\n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"content\",\n",
    "        history_messages_key=\"messages\"\n",
    "    )\n",
    "\n",
    "    return chain_with_history, history_store\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run an interactive chat with persistent SQLite history.\"\"\"\n",
    "    setup_environment()\n",
    "\n",
    "    model_name = \"llama3.2:3b\"\n",
    "    chain_with_history, history_store = create_chat_chain(model_name)\n",
    "\n",
    "    print(\"Start chatting (type 'exit' to quit):\")\n",
    "    session_id = \"default_session\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\">> \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Run the chain\n",
    "            response = chain_with_history.invoke(\n",
    "                {\"content\": user_input, \"messages\": []},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "\n",
    "            print(response.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb81016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
