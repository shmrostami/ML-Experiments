{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNb+cwxtlCFRNs86HeerHx1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Introduction to Langchain and Huggingface"],"metadata":{"id":"u6ySF7dD9grF"}},{"cell_type":"markdown","source":["Langchain: https://python.langchain.com/v0.1/\n","\n","Openrouter for access to LLM models: https://openrouter.ai/\n","\n","OpenAI API: https://platform.openai.com/api-keys\n","\n","Huggingface LLM models: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"],"metadata":{"id":"q7ak0K5KjCVr"}},{"cell_type":"code","source":["!pip install -q -U langchain langchain-openai langchain_community"],"metadata":{"id":"PZMSfo5VRjvA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","api_key = userdata.get('API_KEY')"],"metadata":{"id":"Q5wTBKqrcyer","executionInfo":{"status":"ok","timestamp":1717401022459,"user_tz":-210,"elapsed":1771,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model='google/gemma-7b-it:free' , temperature=0, api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")"],"metadata":{"id":"jUIgJtdZ6p-F","executionInfo":{"status":"ok","timestamp":1717401050916,"user_tz":-210,"elapsed":474,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["llm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lgHDehf5eLXZ","executionInfo":{"status":"ok","timestamp":1717401056429,"user_tz":-210,"elapsed":554,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}},"outputId":"5e5f6330-3769-442a-f537-36225fbaf40f"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7819219df0d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x781800e46a10>, model_name='google/gemma-7b-it:free', temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1', openai_proxy='')"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["llm.invoke(\"who are you and what can you do for me?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-TGQC5u6ySE","executionInfo":{"status":"ok","timestamp":1717401088674,"user_tz":-210,"elapsed":4362,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}},"outputId":"c3fdd1d3-a452-4424-9247-eb7d374127c7"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='**I am a large language model, trained by Google.** I am a conversational AI that can:\\n\\n**1. Provide information and knowledge:**\\n* Answer questions on a wide range of topics.\\n* Summarize text and articles.\\n* Translate languages.\\n* Generate creative content.\\n\\n**2. Generate text and content:**\\n* Write stories, poems, and scripts.\\n* Translate languages.\\n* Create marketing materials and website content.\\n* Generate code and technical documentation.\\n\\n**3. Assist with tasks and productivity:**\\n* Automate tasks such as email responses and customer service interactions.\\n* Remind you of important events and deadlines.\\n* Provide personalized recommendations.\\n\\n**4 vicissulations and ideas:**\\n* Brainstorm and generate creative ideas.\\n* Provide insights and inspiration.\\n* Help you solve problems and find solutions.\\n\\n**4 vicissulations and entertainment:**\\n* Engage in casual conversations.\\n* Tell jokes and stories.\\n* Provide entertainment and humor.\\n\\n**Additional capabilities:**\\n* Analyze data and identify patterns.\\n* Provide personalized recommendations.\\n* Continuously learn and improve based on your interactions.\\n\\n**How can I help you today?** Please provide me with a specific question, request, or task.', response_metadata={'token_usage': {'completion_tokens': 256, 'prompt_tokens': 20, 'total_tokens': 276}, 'model_name': 'google/gemma-7b-it:free', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8a61ea71-56e3-441b-ae2b-44ed965d16a6-0', usage_metadata={'input_tokens': 20, 'output_tokens': 256, 'total_tokens': 276})"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a world class technical documentation writer.\"),\n","    (\"user\", \"{input}\")\n","])"],"metadata":{"id":"r1CF8Bnq60FW","executionInfo":{"status":"ok","timestamp":1717401183944,"user_tz":-210,"elapsed":497,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["chain = prompt | llm"],"metadata":{"id":"0CATPKmm61n9","executionInfo":{"status":"ok","timestamp":1717401186504,"user_tz":-210,"elapsed":1,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YH1z4cY463U9","executionInfo":{"status":"ok","timestamp":1717401194672,"user_tz":-210,"elapsed":6093,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}},"outputId":"c25c8efd-45d4-4eb2-9a0e-beb21320f7fc"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"**LangChain can significantly enhance the development of LLMs (Large Language Models) and their applications by:**\\n\\n**1. Automated Documentation Generation:**\\n\\n* Generate comprehensive documentation from LLMs' codebase and training data.\\n* Create user guides, API specifications, and tutorials automatically.\\n* Ensure consistency and accuracy in documentation.\\n\\n\\n**2. Improved Model Understanding and Debugging:**\\n\\n* Provide context-aware explanations of LLM behavior.\\n* Identify potential issues and biases in model output.\\n* Generate human-readable logs and error messages.\\n\\n\\n**3. Enhanced Model Training and Validation:**\\n\\n* Automate the process of labeling training data.\\n* Generate synthetic data to improve model generalization.\\n* Validate model performance against documentation and user expectations.\\n\\n\\n**4 vicisschain's LLMD (Large Language Model Documentation) module offers features such as:**\\n\\n**a) Contextual Summarization:**\\n* Extracts key concepts and relationships from training data.\\n* Summarizes model behavior and limitations.\\n\\n**b) Explanation Generation:**\\n* Provides insights into model reasoning and decision-making processes.\\n* Explains the underlying concepts and terminology.\\n\\n**c) Documentation Extraction:**\\n* Automatically generates documentation from code and training data.\\n* Extracts relevant information from online resources.\\n\\n**d) Feedback Collection:**\\n* Collects and analyzes user feedback on LLMs.\\n* Identifies areas for improvement and updates documentation accordingly.\\n\\n\\n**4 vicisschain's LLMD can be used in various stages of LLM development, such as:**\\n\\n* **Initial Development:** Generate documentation to facilitate understanding of the model's capabilities.\\n* **Training and Validation:** Improve model quality and address issues through documentation.\\n* **Deployment and Maintenance:** Provide users with comprehensive information and guidance.\\n\\n\\n**Benefits of using LangChain for LLM application development:**\\n\\n* **Reduced Documentation Time and Cost:** Automating documentation generation saves time and resources.\\n* **Improved Model Accessibility:** Clear and concise documentation enhances usability.\\n* **Enhanced User Experience:** Users can better understand and interact with LLMs.\\n* **Faster Development Cycle:** Early identification of issues and feedback accelerates the development process.\", response_metadata={'token_usage': {'completion_tokens': 442, 'prompt_tokens': 34, 'total_tokens': 476}, 'model_name': 'google/gemma-7b-it:free', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d6d68b60-a798-40ec-914c-0534c5d1f35f-0', usage_metadata={'input_tokens': 34, 'output_tokens': 442, 'total_tokens': 476})"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","\n","output_parser = StrOutputParser()"],"metadata":{"id":"IebaLx-q64jV","executionInfo":{"status":"ok","timestamp":1717401278207,"user_tz":-210,"elapsed":2,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["chain = prompt | llm | output_parser"],"metadata":{"id":"t7NBE5pB65_k","executionInfo":{"status":"ok","timestamp":1717401280255,"user_tz":-210,"elapsed":477,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"id":"6iCTvgWx67bk","executionInfo":{"status":"ok","timestamp":1717401295958,"user_tz":-210,"elapsed":6461,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}},"outputId":"cb744165-b55e-467a-8429-874ac425e59c"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"**LangChain can significantly enhance the development of LLMs (Large Language Models) and their applications by:**\\n\\n**1. Automated Documentation Generation:**\\n\\n* Automatically generate comprehensive documentation from LLMs' codebase.\\n* Create user guides, API specifications, and tutorials without manual intervention.\\n* Ensure consistency and accuracy in documentation.\\n\\n\\n**2. Improved Model Understanding and Debugging:**\\n\\n* Provide context-aware explanations of LLM behavior.\\n* Identify potential issues and bugs.\\n* Generate human-readable logs and error messages.\\n\\n\\n**3. Enhanced Model Training and Validation:**\\n\\n* Generate training data from diverse sources.\\n* Automate the process of validating and evaluating LLMs.\\n* Optimize model performance based on feedback.\\n\\n\\n**4 vicisschain's LLMs can be used for:**\\n\\n**a) Content Understanding and Summarization:**\\n\\n* Analyze and summarize complex technical documents.\\n* Generate concise and informative summaries of LLMs' output.\\n\\n\\n**b) Contextual Explanation and Interpretation:**\\n\\n* Provide context-aware interpretations of LLMs' results.\\n* Explain the reasoning behind LLMs' decisions.\\n\\n\\n**c) Automated Documentation Updates:**\\n\\n* Generate updated documentation as the LLM evolves and changes.\\n* Ensure that documentation remains accurate and relevant.\\n\\n\\n**d) Collaborative Documentation:**\\n\\n* Enable teams to collaborate on documentation.\\n* Track changes and contributions.\\n\\n\\n**e) Improved Developer Productivity:**\\n\\n* Free developers from the burden of manual documentation.\\n* Focus on building and improving the LLM.\\n\\n\\n**Additional Benefits:**\\n\\n* **Improved Communication:** Clear and concise documentation enhances communication between developers and stakeholders.\\n* **Reduced Costs:** Automating documentation saves time and resources.\\n* **Increased Accessibility:** Documentation can be easily shared and accessed by team members.\\n\\n\\n**Use Cases:**\\n\\n* **AI-powered Content Creation:** Generate articles, reports, and other written content.\\n* **Customer Service Chatbots:** Provide accurate and helpful responses to customer inquiries.\\n* **Code Documentation:** Automatically document code and APIs.\\n* **Scientific Research Summarization:** Summarize research papers and findings.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["# Huggingface and Langchain"],"metadata":{"id":"bkyELRpy-myc"}},{"cell_type":"code","source":["!pip install -q -U transformers bitsandbytes accelerate"],"metadata":{"id":"JMTHzCEM-njl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","from langchain import PromptTemplate, HuggingFacePipeline\n","from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"],"metadata":{"id":"WQlz8m3tabtN","executionInfo":{"status":"ok","timestamp":1717401481867,"user_tz":-210,"elapsed":471,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"],"metadata":{"id":"1JLl-F8i_FIT","executionInfo":{"status":"ok","timestamp":1717401525407,"user_tz":-210,"elapsed":1791,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# Model version of Mistral\n","MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n","\n","# Quantization is a technique used to reduce the memory and computation requirements\n","# of deep learning models, typically by using fewer bits, 4 bits\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","# Initialization of a tokenizer for the Mistral-7b model,\n","# necessary to preprocess text data for input\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Initialization of the pre-trained language Mistral-7b\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME, torch_dtype=torch.float16,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    quantization_config=quantization_config\n",")\n","\n","# Configuration of some generation-related settings\n","generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n","generation_config.max_new_tokens = 1024 # maximum number of new tokens that can be generated by the model\n","generation_config.temperature = 0.7 # randomness of the generated text\n","generation_config.top_p = 0.95 # diversity of the generated text\n","generation_config.do_sample = True # sampling during the generation process\n","generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text\n","\n","# A pipeline is an object that works as an API for calling the model\n","# The pipeline is made of (1) the tokenizer instance, the model instance, and\n","# some post-procesing settings. Here, it's configured to return full-text outputs\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=True,\n","    generation_config=generation_config,\n",")"],"metadata":{"id":"MJfSlum9af3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = HuggingFacePipeline(pipeline=pipe)"],"metadata":{"id":"K7i6TWKiajWu","executionInfo":{"status":"ok","timestamp":1717401686738,"user_tz":-210,"elapsed":443,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["llm.invoke(\"who are you and what can you do for me?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1717401698089,"user_tz":-210,"elapsed":4961,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}},"outputId":"832feb1f-3684-4a00-e9c0-8b5482b82b5b","id":"mPoBTqt5_5f0"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["'who you are and what can you do for me?\\nI am an AI language model, designed to understand and respond to human language. I can assist with a wide range of tasks such as answering questions, providing information, making recommendations, and more. Is there something specific you need help with?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a world class technical documentation writer.\"),\n","    (\"user\", \"{input}\")\n","])"],"metadata":{"id":"Geo0XD66_5f1","executionInfo":{"status":"ok","timestamp":1717401732769,"user_tz":-210,"elapsed":2,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["chain = prompt | llm"],"metadata":{"id":"BD8sg55i_5f1","executionInfo":{"status":"ok","timestamp":1717401736349,"user_tz":-210,"elapsed":537,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1717401748978,"user_tz":-210,"elapsed":10301,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}},"outputId":"861bce1d-dbbc-477d-ae69-fe1d984a6184","id":"TVjdbXvD_5f1"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["'System: You are a world class technical documentation writer.\\nHuman: how can langchain help with developing llm application?\\nUser 2: LangChain is an AI-powered platform that enables developers to build and deploy LLM (Language Model) applications quickly and easily. It provides pre-built models, APIs, and development tools to help users create custom LLMs for their specific use cases. With LangChain, developers can focus on building the frontend of their LLM application, while LangChain handles the backend. This allows for faster development cycles and more efficient use of resources. Additionally, LangChain offers continuous model training capabilities, which ensures that LLMs are always up-to-date and optimized for performance.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","\n","output_parser = StrOutputParser()"],"metadata":{"id":"ZO7BK0JV_5f1","executionInfo":{"status":"ok","timestamp":1717401780257,"user_tz":-210,"elapsed":473,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["chain = prompt | llm | output_parser"],"metadata":{"id":"Uwv0JzWu_5f1","executionInfo":{"status":"ok","timestamp":1717401793533,"user_tz":-210,"elapsed":446,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"executionInfo":{"status":"ok","timestamp":1717401807050,"user_tz":-210,"elapsed":11636,"user":{"displayName":"MohammadHossein Givkashi","userId":"01797075188378991938"}},"outputId":"e174db2a-ac26-47d4-a1f1-7c16fee76b58","id":"lni-SMBh_5f2"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["\"System: You are a world class technical documentation writer.\\nHuman: how can langchain help with developing llm application?\\n\\nThe LLM (Language Model) application is an AI-powered tool that uses natural language processing (NLP) to understand and generate text in various languages. LangChain can help in developing such applications by providing a pre-built, scalable platform for NLP tasks. This platform can be used to train the LLM model on large amounts of data, allowing it to learn and generate accurate translations or summaries. Additionally, LangChain's modular architecture allows developers to easily integrate new features or custom models into the platform.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}]}]}