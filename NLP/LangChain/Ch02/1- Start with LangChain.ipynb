{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6ySF7dD9grF"
   },
   "source": [
    "# Introduction to Langchain and Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7ak0K5KjCVr"
   },
   "source": [
    "Langchain: https://python.langchain.com/docs/introduction/\n",
    "\n",
    "Openrouter for access to LLM models: https://openrouter.ai/\n",
    "\n",
    "OpenAI API: https://platform.openai.com/api-keys\n",
    "\n",
    "Huggingface LLM models: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PZMSfo5VRjvA"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U langchain langchain-openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1717401022459,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "Q5wTBKqrcyer"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "# api_key = userdata.get('API_KEY')\n",
    "api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1717401050916,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "jUIgJtdZ6p-F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000217B60C81D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000217B60CA690>, root_client=<openai.OpenAI object at 0x00000217B5CC81D0>, root_async_client=<openai.AsyncOpenAI object at 0x00000217B60C8230>, model_name='google/gemini-2.0-flash-thinking-exp:free', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='google/gemini-2.0-flash-thinking-exp:free' , temperature=0, api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4362,
     "status": "ok",
     "timestamp": 1717401088674,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "x-TGQC5u6ySE",
    "outputId": "c3fdd1d3-a452-4424-9247-eb7d374127c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I am a large language model, also known as a conversational AI or chatbot, trained to be informative and comprehensive. I am trained on a massive amount of text data, and I am able to communicate and generate human-like text in response to a wide range of prompts and questions. For example, I can provide summaries of factual topics, create stories, and translate languages.\\n\\nThink of me as a helpful and versatile digital assistant.  Here's a breakdown of what I can do for you:\\n\\n**Information and Knowledge:**\\n\\n* **Answer your questions:** I can access and process information from the real world through Google Search and keep my response consistent with search results.  Ask me anything you're curious about, from historical facts to scientific concepts to current events.\\n* **Explain complex topics:**  I can break down complicated subjects into easier-to-understand explanations.\\n* **Provide summaries:**  Need a quick overview of a long article or document? I can summarize it for you.\\n* **Define terms and concepts:**  Unsure about the meaning of a word or phrase? I can provide definitions.\\n* **Offer different perspectives:**  I can explore topics from various angles and present different viewpoints.\\n\\n**Creative Tasks and Content Generation:**\\n\\n* **Write different kinds of creative content:** I can help you write stories, poems, scripts, musical pieces, email, letters, etc.\\n* **Brainstorm ideas:**  Stuck on a project? I can help you generate ideas and explore different possibilities.\\n* **Outline projects and tasks:**  Need help structuring your thoughts? I can create outlines for essays, presentations, or even just your to-do list.\\n* **Translate languages:**  I can translate text between multiple languages.\\n* **Generate different creative text formats,**  like code, scripts, musical pieces, email, letters, etc.\\n\\n**Assistance and Productivity:**\\n\\n* **Help you write and edit:**  I can assist you with writing tasks, offering suggestions for grammar, style, and clarity.\\n* **Organize information:**  I can help you structure and organize your thoughts and information.\\n* **Provide different creative text formats,** like poems, code, scripts, musical pieces, email, letters, etc.\\n* **Offer suggestions and recommendations:**  Based on your requests, I can offer suggestions for books, movies, restaurants, and more (though remember to verify information, especially for real-world things).\\n\\n**Learning and Exploration:**\\n\\n* **Act as a learning tool:**  I can help you learn about new subjects and explore different areas of interest.\\n* **Provide examples and illustrations:**  I can give you concrete examples to help you understand abstract concepts.\\n* **Offer different perspectives on a topic:**  I can help you see things from different angles and broaden your understanding.\\n\\n**Important things to keep in mind:**\\n\\n* **I am not a human:**  While I can communicate in a human-like way, I don't have feelings, opinions, or personal experiences.\\n* **My knowledge is based on my training data:**  I am constantly being updated, but my knowledge is still limited to the information I was trained on.  I may not always have the most up-to-date information on very recent events.\\n* **I can make mistakes:**  While I strive for accuracy, I am still under development and can sometimes generate incorrect or nonsensical information.  **Always double-check critical information, especially if it's important for your health, finances, or safety.**\\n* **I am not a substitute for professional advice:**  I can provide information, but I cannot give medical, legal, financial, or other professional advice.  Always consult with a qualified professional for those needs.\\n\\n**In short, I am here to assist you with a wide range of tasks that involve information, creativity, and communication.  Tell me what you need, and I'll do my best to help!**\\n\\n**So, what can I do for *you* today?  What are you interested in or working on?**  Let me know how I can be of service. ðŸ˜Š\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1804, 'prompt_tokens': 12, 'total_tokens': 1816, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'google/gemini-2.0-flash-thinking-exp', 'system_fingerprint': None, 'id': 'gen-1744098285-o7C5FfTgRM4bHn5QI7vR', 'finish_reason': 'stop', 'logprobs': None}, id='run-ac99aa89-544e-4ef3-b5dc-ddc77733edbc-0', usage_metadata={'input_tokens': 12, 'output_tokens': 1804, 'total_tokens': 1816, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"who are you and what can you do for me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1717401183944,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "r1CF8Bnq60FW"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1717401186504,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "0CATPKmm61n9"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langchain is a powerful framework that significantly simplifies and accelerates the development of applications powered by Large Language Models (LLMs). It acts as a bridge, providing tools and abstractions that make it easier to interact with, orchestrate, and build complex applications on top of LLMs like GPT-3, PaLM, and others.\\n\\nHere\\'s a breakdown of how Langchain helps with developing LLM applications, categorized by key areas:\\n\\n**1. Simplifying LLM Interaction and Abstraction:**\\n\\n* **Unified Interface for Multiple LLMs:** Langchain provides a consistent interface to interact with various LLM providers (OpenAI, Cohere, Hugging Face, etc.). You don\\'t need to learn the specific API details for each provider.  This allows you to easily switch between models or even use ensembles of models without rewriting significant portions of your code.\\n* **Abstraction over LLM APIs:** Langchain handles the low-level details of interacting with LLM APIs, such as authentication, request formatting, and response parsing. This lets you focus on the logic of your application rather than the intricacies of API calls.\\n* **Simplified Prompt Management:**  Langchain offers tools for creating, managing, and versioning prompts.  `PromptTemplates` allow you to define reusable prompt structures with placeholders for dynamic inputs, making prompt engineering more organized and efficient.\\n* **Output Parsing:** LLMs often return text, but you usually need structured data. Langchain provides `OutputParsers` to help you reliably extract structured information (like JSON, lists, or specific data types) from LLM responses, making it easier to integrate LLM outputs into your application logic.\\n\\n**2. Building Complex LLM Workflows and Chains:**\\n\\n* **Chaining LLM Calls:** Langchain\\'s core concept is \"Chains.\" These are sequences of calls to LLMs or other utilities. You can chain together different components (LLMs, prompts, utilities, memories) to create sophisticated workflows. This allows you to build applications that go beyond single LLM queries and perform multi-step reasoning, data retrieval, and action execution.\\n* **Pre-built Chains for Common Tasks:** Langchain offers a library of pre-built chains for common LLM application patterns, such as:\\n    * **Retrieval-Augmented Generation (RAG):** Chains for combining LLMs with external knowledge bases (vector databases, document stores) to answer questions based on specific data.\\n    * **Question Answering:** Chains designed for answering questions based on provided context or documents.\\n    * **Summarization:** Chains for condensing large amounts of text into summaries.\\n    * **Conversational Chains:** Chains that incorporate memory to maintain context in conversations.\\n* **Custom Chain Creation:** You can easily create custom chains by combining Langchain\\'s components or even integrating your own custom logic. This flexibility allows you to tailor workflows to the specific needs of your application.\\n\\n**3. Enhancing LLM Capabilities with Memory and Context:**\\n\\n* **Memory Management:**  For conversational applications, maintaining context is crucial. Langchain provides various `Memory` modules that allow you to store and retrieve conversation history. This enables your LLM application to remember past interactions and provide more coherent and context-aware responses. Different memory types cater to different needs (e.g., buffer memory, token window memory, vector store memory).\\n* **Contextual Awareness:** By integrating memory and retrieval mechanisms, Langchain helps you build applications that are aware of their past interactions and external knowledge, leading to more engaging and useful user experiences.\\n\\n**4. Enabling Agents and Tool Use:**\\n\\n* **Agents for Autonomous Decision-Making:** Langchain\\'s `Agents` framework allows LLMs to become autonomous decision-makers. Agents can use a set of `Tools` (functions or external resources) to interact with the environment, gather information, and perform actions based on user requests.\\n* **Tool Integration:** Langchain provides a wide range of pre-built tools for tasks like:\\n    * **Web Search:**  Searching the internet for information.\\n    * **Calculations:** Performing mathematical operations.\\n    * **File System Access:** Reading and writing files.\\n    * **Database Interaction:** Querying databases.\\n    * **APIs:** Interacting with external APIs.\\n* **Building Interactive and Dynamic Applications:** Agents and tools empower you to create LLM applications that are not just passive text generators but can actively interact with the world, making them suitable for tasks like personal assistants, automated workflows, and more.\\n\\n**5. Data Handling and Integration:**\\n\\n* **Document Loaders:** Langchain provides `Document Loaders` to easily ingest data from various sources, including:\\n    * Text files, PDFs, Word documents, Markdown\\n    * Websites, APIs, Databases\\n    * Cloud storage (S3, Google Cloud Storage)\\n* **Text Splitting:**  LLMs have token limits. Langchain offers `Text Splitters` to break down large documents into smaller chunks that fit within these limits while preserving semantic meaning. This is crucial for processing long documents for tasks like question answering or summarization.\\n* **Vector Databases and Embeddings:** For RAG applications, Langchain integrates seamlessly with vector databases (like Chroma, Pinecone, FAISS, etc.). It provides tools for:\\n    * Generating embeddings (vector representations) of text using embedding models (OpenAI Embeddings, Hugging Face Transformers).\\n    * Storing and querying embeddings in vector databases for efficient semantic search and retrieval.\\n\\n**6. Evaluation and Testing:**\\n\\n* **Evaluation Frameworks:** Langchain provides tools and frameworks to evaluate the performance of your LLM applications. This includes metrics for:\\n    * **Faithfulness:** How well the LLM response aligns with the provided context.\\n    * **Relevance:** How relevant the LLM response is to the user\\'s query.\\n    * **Correctness:**  Accuracy of factual information in the response.\\n* **Iterative Development:** Evaluation tools help you identify areas for improvement in your prompts, chains, and overall application design, enabling an iterative development process.\\n\\n**7. Community and Ecosystem:**\\n\\n* **Active Community:** Langchain has a vibrant and active community, providing support, examples, and contributions. This makes it easier to learn, troubleshoot, and stay up-to-date with the latest developments.\\n* **Rapid Development and Updates:** Langchain is under active development, with frequent updates and new features being added. This ensures you have access to the latest advancements in the LLM space.\\n* **Integration with Other Tools:** Langchain integrates well with other popular Python libraries and tools in the AI/ML ecosystem, making it a versatile framework for building complex applications.\\n\\n**In Summary, Langchain helps you develop LLM applications by:**\\n\\n* **Reducing complexity:** Abstracting away low-level details of LLM APIs and providing high-level abstractions.\\n* **Increasing efficiency:** Offering pre-built components and chains for common tasks, saving development time.\\n* **Enhancing capabilities:** Enabling memory, tool use, and complex workflows to build more sophisticated applications.\\n* **Improving data handling:** Providing tools for loading, processing, and integrating data from various sources.\\n* **Facilitating evaluation:** Offering frameworks for assessing and improving application performance.\\n\\nBy leveraging Langchain, developers can focus on the core logic and user experience of their LLM applications, rather than getting bogged down in the technical intricacies of interacting with LLMs directly. It significantly lowers the barrier to entry for building powerful and innovative applications powered by the latest advancements in natural language processing.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2724, 'prompt_tokens': 21, 'total_tokens': 2745, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'google/gemini-2.0-flash-thinking-exp', 'system_fingerprint': None, 'id': 'gen-1744098362-jk5iCup6m37REbq1Y2w3', 'finish_reason': 'stop', 'logprobs': None}, id='run-5158443b-1883-4b49-b12c-c6bb46c8e2f9-0', usage_metadata={'input_tokens': 21, 'output_tokens': 2724, 'total_tokens': 2745, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1717401278207,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "IebaLx-q64jV"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1717401280255,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "t7NBE5pB65_k"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "executionInfo": {
     "elapsed": 6461,
     "status": "ok",
     "timestamp": 1717401295958,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "6iCTvgWx67bk",
    "outputId": "cb744165-b55e-467a-8429-874ac425e59c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a powerful framework that significantly simplifies and accelerates the development of applications powered by Large Language Models (LLMs). It acts as a bridge, providing tools and abstractions that make it easier to interact with, orchestrate, and build complex applications on top of LLMs like GPT-3, PaLM, and others.\\n\\nHere\\'s a breakdown of how Langchain helps with developing LLM applications, categorized by key areas:\\n\\n**1. Simplifying LLM Interaction and Abstraction:**\\n\\n* **Unified Interface for Multiple LLMs:** Langchain provides a consistent interface to interact with various LLM providers (OpenAI, Cohere, Hugging Face, etc.). You don\\'t need to learn the specific API details for each provider.  This allows you to easily switch between models or even use ensembles of models without rewriting significant portions of your code.\\n* **Abstraction over LLM APIs:** Langchain handles the low-level details of interacting with LLM APIs, such as authentication, request formatting, and response parsing. This lets you focus on the logic of your application rather than the intricacies of API calls.\\n* **Simplified Prompt Management:**  Langchain offers tools for creating, managing, and versioning prompts.  `PromptTemplates` allow you to define reusable prompt structures with placeholders for dynamic inputs, making prompt engineering more organized and efficient.\\n* **Output Parsing:** LLMs often return text, but you usually need structured data. Langchain provides `OutputParsers` to help you reliably extract structured information (like JSON, lists, or specific data types) from LLM responses, making it easier to integrate LLM outputs into your application logic.\\n\\n**2. Building Complex LLM Workflows and Chains:**\\n\\n* **Chaining LLM Calls:** Langchain\\'s core concept is \"Chains.\" These are sequences of calls to LLMs or other utilities. You can chain together different components (LLMs, prompts, utilities, memories) to create sophisticated workflows. This allows you to build applications that go beyond single LLM queries and perform multi-step reasoning, data retrieval, and action execution.\\n* **Pre-built Chains for Common Tasks:** Langchain offers a library of pre-built chains for common LLM application patterns, such as:\\n    * **Retrieval-Augmented Generation (RAG):** Chains for combining LLMs with external knowledge bases (vector databases, document stores) to answer questions based on specific data.\\n    * **Question Answering:** Chains designed for answering questions based on provided context or documents.\\n    * **Summarization:** Chains for condensing large amounts of text into summaries.\\n    * **Conversational Chains:** Chains that incorporate memory to maintain context in conversations.\\n* **Custom Chain Creation:** You can easily create custom chains by combining Langchain\\'s components or even integrating your own custom logic. This flexibility allows you to tailor workflows to the specific needs of your application.\\n\\n**3. Enhancing LLM Capabilities with Memory and Context:**\\n\\n* **Memory Management:**  For conversational applications, maintaining context is crucial. Langchain provides various `Memory` modules that allow you to store and retrieve conversation history. This enables your LLM application to remember past interactions and provide more coherent and context-aware responses. Different memory types cater to different needs (e.g., buffer memory, token window memory, vector store memory).\\n* **Contextual Awareness:** By integrating memory and retrieval mechanisms, Langchain helps you build applications that are aware of their past interactions and external knowledge, leading to more engaging and useful user experiences.\\n\\n**4. Enabling Agents and Tool Use:**\\n\\n* **Agents for Autonomous Decision-Making:** Langchain\\'s `Agents` framework allows LLMs to become autonomous decision-makers. Agents can use a set of `Tools` (functions or external resources) to interact with the environment, gather information, and perform actions based on user requests.\\n* **Tool Integration:** Langchain provides a wide range of pre-built tools for tasks like:\\n    * **Web Search:**  Searching the internet for information.\\n    * **Calculations:** Performing mathematical operations.\\n    * **File System Access:** Reading and writing files.\\n    * **Database Interaction:** Querying databases.\\n    * **APIs:** Interacting with external APIs.\\n* **Building Interactive and Dynamic Applications:** Agents and tools empower you to create LLM applications that are not just passive text generators but can actively interact with the world, making them suitable for tasks like personal assistants, automated workflows, and more.\\n\\n**5. Data Handling and Integration:**\\n\\n* **Document Loaders:** Langchain provides `Document Loaders` to easily ingest data from various sources, including:\\n    * Text files, PDFs, Word documents, Markdown\\n    * Websites, APIs, Databases\\n    * Cloud storage (S3, Google Cloud Storage)\\n* **Text Splitting:**  LLMs have token limits. Langchain offers `Text Splitters` to break down large documents into smaller chunks that fit within these limits while preserving semantic meaning. This is crucial for processing long documents for tasks like question answering or summarization.\\n* **Vector Databases and Embeddings:** For RAG applications, Langchain integrates seamlessly with vector databases (like Chroma, Pinecone, FAISS, etc.). It provides tools for:\\n    * Generating embeddings (vector representations) of text using embedding models (OpenAI Embeddings, Hugging Face Transformers).\\n    * Storing and querying embeddings in vector databases for efficient semantic search and retrieval.\\n\\n**6. Evaluation and Testing:**\\n\\n* **Evaluation Frameworks:** Langchain provides tools and frameworks to evaluate the performance of your LLM applications. This includes metrics for:\\n    * **Faithfulness:** How well the LLM response aligns with the provided context.\\n    * **Relevance:** How relevant the LLM response is to the user\\'s query.\\n    * **Correctness:**  Accuracy of factual information in the response.\\n* **Iterative Development:** Evaluation tools help you identify areas for improvement in your prompts, chains, and overall application design, enabling an iterative development process.\\n\\n**7. Community and Ecosystem:**\\n\\n* **Active Community:** Langchain has a vibrant and active community, providing support, examples, and contributions. This makes it easier to learn, troubleshoot, and stay up-to-date with the latest developments.\\n* **Rapid Development and Updates:** Langchain is under active development, with frequent updates and new features being added. This ensures you have access to the latest advancements in the LLM space.\\n* **Integration with Other Tools:** Langchain integrates well with other popular Python libraries and tools in the AI/ML ecosystem, making it a versatile framework for building complex applications.\\n\\n**In Summary, Langchain helps you develop LLM applications by:**\\n\\n* **Reducing complexity:** Abstracting away low-level details of LLM APIs and providing high-level abstractions.\\n* **Increasing efficiency:** Offering pre-built components and chains for common tasks, saving development time.\\n* **Enhancing capabilities:** Enabling memory, tool use, and complex workflows to build more sophisticated applications.\\n* **Improving data handling:** Providing tools for loading, processing, and integrating data from various sources.\\n* **Facilitating evaluation:** Offering frameworks for assessing and improving application performance.\\n\\nBy leveraging Langchain, developers can focus on the core logic and user experience of their LLM applications, rather than getting bogged down in the technical intricacies of interacting with LLMs directly. It significantly lowers the barrier to entry for building powerful and innovative applications powered by the latest advancements in natural language processing.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkyELRpy-myc"
   },
   "source": [
    "# Huggingface and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JMTHzCEM-njl"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1717401481867,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "WQlz8m3tabtN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hossein\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from langchain import PromptTemplate, HuggingFacePipeline\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 1791,
     "status": "ok",
     "timestamp": 1717401525407,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "1JLl-F8i_FIT"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJfSlum9af3p"
   },
   "outputs": [],
   "source": [
    "# Model version of Mistral\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Quantization is a technique used to reduce the memory and computation requirements\n",
    "# of deep learning models, typically by using fewer bits, 4 bits\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Initialization of a tokenizer for the Mistral-7b model,\n",
    "# necessary to preprocess text data for input\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialization of the pre-trained language Mistral-7b\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Configuration of some generation-related settings\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1024 # maximum number of new tokens that can be generated by the model\n",
    "generation_config.temperature = 0.7 # randomness of the generated text\n",
    "generation_config.top_p = 0.95 # diversity of the generated text\n",
    "generation_config.do_sample = True # sampling during the generation process\n",
    "generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text\n",
    "\n",
    "# A pipeline is an object that works as an API for calling the model\n",
    "# The pipeline is made of (1) the tokenizer instance, the model instance, and\n",
    "# some post-procesing settings. Here, it's configured to return full-text outputs\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1717401686738,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "K7i6TWKiajWu"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 4961,
     "status": "ok",
     "timestamp": 1717401698089,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "mPoBTqt5_5f0",
    "outputId": "832feb1f-3684-4a00-e9c0-8b5482b82b5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'who you are and what can you do for me?\\nI am an AI language model, designed to understand and respond to human language. I can assist with a wide range of tasks such as answering questions, providing information, making recommendations, and more. Is there something specific you need help with?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"who are you and what can you do for me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1717401732769,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "Geo0XD66_5f1"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1717401736349,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "BD8sg55i_5f1"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "executionInfo": {
     "elapsed": 10301,
     "status": "ok",
     "timestamp": 1717401748978,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "TVjdbXvD_5f1",
    "outputId": "861bce1d-dbbc-477d-ae69-fe1d984a6184"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'System: You are a world class technical documentation writer.\\nHuman: how can langchain help with developing llm application?\\nUser 2: LangChain is an AI-powered platform that enables developers to build and deploy LLM (Language Model) applications quickly and easily. It provides pre-built models, APIs, and development tools to help users create custom LLMs for their specific use cases. With LangChain, developers can focus on building the frontend of their LLM application, while LangChain handles the backend. This allows for faster development cycles and more efficient use of resources. Additionally, LangChain offers continuous model training capabilities, which ensures that LLMs are always up-to-date and optimized for performance.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1717401780257,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "ZO7BK0JV_5f1"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1717401793533,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "Uwv0JzWu_5f1"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "executionInfo": {
     "elapsed": 11636,
     "status": "ok",
     "timestamp": 1717401807050,
     "user": {
      "displayName": "MohammadHossein Givkashi",
      "userId": "01797075188378991938"
     },
     "user_tz": -210
    },
    "id": "lni-SMBh_5f2",
    "outputId": "e174db2a-ac26-47d4-a1f1-7c16fee76b58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"System: You are a world class technical documentation writer.\\nHuman: how can langchain help with developing llm application?\\n\\nThe LLM (Language Model) application is an AI-powered tool that uses natural language processing (NLP) to understand and generate text in various languages. LangChain can help in developing such applications by providing a pre-built, scalable platform for NLP tasks. This platform can be used to train the LLM model on large amounts of data, allowing it to learn and generate accurate translations or summaries. Additionally, LangChain's modular architecture allows developers to easily integrate new features or custom models into the platform.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNb+cwxtlCFRNs86HeerHx1",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
